{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "nielsen15 = pd.read_csv('../../Nielsen/aggregated_nielsen_2015.csv')\n",
    "nielsen16 = pd.read_csv('../..//Nielsen/aggregated_nielsen_2016.csv')\n",
    "nielsen15['year'] = 2015\n",
    "nielsen16['year'] = 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "nielsen = pd.concat((nielsen15, nielsen16))\n",
    "nielsen = nielsen[~nielsen.is_walmart]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Entriy/exit dates\n",
    "fandom = pd.read_csv('../data_collection/plein_de_data/fandom_traitÃ©es.csv', parse_dates=['Opening_date', 'Closing_date'])[['State', 'County_name', 'County_fips', 'Opening_date', 'Closing_date']]\n",
    "\n",
    "# We drop the state in which we do not trust our data (some mistakes stillremain)\n",
    "fandom = fandom[~np.isin(fandom.State, ('CA', 'GA', 'KS', 'LA', 'TX'))]\n",
    "nielsen = nielsen[~np.isin(nielsen.store_state, ('CA', 'GA', 'KS', 'LA', 'TX'))]\n",
    "\n",
    "# We concentrate our study on the movements (entries & exits) during the fiscal years 2015 and 2016\n",
    "#movements = fandom[((fandom.Opening_date >= '2015-01-31') & (fandom.Opening_date <= '2017-01-31')) | ((fandom.Closing_date >= '2015-01-31') & (fandom.Closing_date <= '2017-01-31'))]\n",
    "# Correction des effets de bord M12\n",
    "movements = fandom[((fandom.Opening_date >= '2014-01-31') & (fandom.Opening_date <= '2018-01-31')) | ((fandom.Closing_date >= '2014-01-31') & (fandom.Closing_date <= '2018-01-31'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#categories=[\"MILK\"]\n",
    "#nielsen['upc_price'] = nielsen.price_per_gal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "hey = nielsen.groupby('product_group_descr').count()\n",
    "categories = hey[hey.is_walmart<25000].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "a = [    \"FRESH PRODUCE\",\n",
    "    \"BREAD AND BAKED GOODS\",\n",
    "    \"MILK\",\n",
    "    \"SNACKS\",\n",
    "    \"PACKAGED MEATS-DELI\",\n",
    "    \"CHEESE\",\n",
    "    \"UNPREP MEAT/POULTRY/SEAFOOD-FRZN\",\n",
    "    \"CARBONATED BEVERAGES\",]\n",
    "b = [\n",
    "    \"CONDIMENTS, GRAVIES, AND SAUCES\",\n",
    "    \"CANDY\",\n",
    "    \"JUICE, DRINKS - CANNED, BOTTLED\",\n",
    "    \"EGGS\",\n",
    "    \"CEREAL\",\n",
    "    \"PASTA\",\n",
    "    \"PAPER PRODUCTS\",\n",
    "    \"SEAFOOD - CANNED\"]\n",
    "\n",
    "categories=[\"SEAFOOD - CANNED\",\n",
    "    \"BEER\",\n",
    "    \"BREAKFAST FOODS - FROZEN\",\n",
    "    \"COOKIES\",\n",
    "    \"COT CHEESE, SOUR CREAM, TOPPINGS\",\n",
    "   \"COUGH AND COLD REMEDIES\",\n",
    "    \"DEODORANT\",\n",
    "    \"DESSERTS/FRUITS/TOPPINGS-FROZEN\",\n",
    "  \"DISPOSABLE DIAPERS\",\n",
    "  \"DRESSINGS/SALADS/PREP FOODS-DELI\",\n",
    "  \"GRT CARDS/PARTY NEEDS/NOVELTIES\",\n",
    "    \"ICE CREAM, NOVELTIES\",\n",
    "  \"JAMS, JELLIES, SPREADS\",\n",
    "  \"PIZZA/SNACKS/HORS DOEUVRES-FRZN\",\n",
    "    \"PREPARED FOOD-DRY MIXES\",\n",
    "    \"PREPARED FOOD-READY-TO-SERVE\",\n",
    "    \"SPICES, SEASONINGS, EXTRACTS\",\n",
    "    \"VEGETABLES - CANNED\",\n",
    "    \"VEGETABLES-FROZEN\",\n",
    "    \"WRAPPING MATERIALS AND BAGS\",\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#categories =[\n",
    "#    \"COT CHEESE, SOUR CREAM, TOPPINGS\",\n",
    "#    \"PACKAGED MILK AND MODIFIERS\",\n",
    "#    \"SPICES, SEASONING, EXTRACTS\",\n",
    "#    \"SUGAR, SWEETENERS\"\n",
    "#]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#categories=[\n",
    "#    \"BAKING MIXES\",\n",
    "#    \"MILK\",\n",
    "#    \"CHEESE\",\n",
    "#    \"COT CHEESE, SOUR CREAM, TOPPINGS\",\n",
    "#    \"CRACKERS\"\n",
    "#    \"CANDY\",\n",
    "##    \"PASTA\",\n",
    "    \n",
    "#]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pool = pd.DataFrame(nielsen.groupby(['is_walmart', 'store_state', 'guessed_store_county', 'guessed_store_county_fips', 'purchase_year', 'purchase_month']).mean()['upc_price']).reset_index()\n",
    "\n",
    "#categories = ['_']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#post = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODELE 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Prices_{i, t} = \\alpha + \\beta treat_i + \\gamma treat_i* post_t + e*post_t$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#categories = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the control group: 122.\n",
      "Size of the control group: 96.\n",
      "Size of the control group: 0.\n",
      "Size of the control group: 316.\n",
      "Size of the control group: 320.\n",
      "Size of the control group: 99.\n",
      "=========================================================\n",
      "COUGH AND COLD REMEDIES 3\n",
      "Coef : 0.07538213685124129\n",
      "Coef/err : 2.0242452384356384\n",
      "=========================================================\n",
      "COUGH AND COLD REMEDIES 6\n",
      "Coef : 0.07866822311256882\n",
      "Coef/err : 2.0213570237055563\n",
      "=========================================================\n",
      "COUGH AND COLD REMEDIES 11\n",
      "Coef : 0.09181070618451104\n",
      "Coef/err : 2.306721418722176\n",
      "Size of the control group: 71.\n",
      "Size of the control group: 93.\n",
      "Size of the control group: 4.\n",
      "Size of the control group: 492.\n",
      "=========================================================\n",
      "DRESSINGS/SALADS/PREP FOODS-DELI 1\n",
      "Coef : 0.03466521897944608\n",
      "Coef/err : 2.759534672183056\n",
      "=========================================================\n",
      "DRESSINGS/SALADS/PREP FOODS-DELI 3\n",
      "Coef : 0.03907663846238196\n",
      "Coef/err : 2.955295927463363\n",
      "=========================================================\n",
      "DRESSINGS/SALADS/PREP FOODS-DELI 4\n",
      "Coef : 0.03409199776328875\n",
      "Coef/err : 2.7239999172316582\n",
      "=========================================================\n",
      "DRESSINGS/SALADS/PREP FOODS-DELI 5\n",
      "Coef : 0.03518758821434176\n",
      "Coef/err : 2.7871911650909498\n",
      "=========================================================\n",
      "DRESSINGS/SALADS/PREP FOODS-DELI 6\n",
      "Coef : 0.03369648114276291\n",
      "Coef/err : 2.6289574848421937\n",
      "=========================================================\n",
      "DRESSINGS/SALADS/PREP FOODS-DELI 7\n",
      "Coef : 0.032552643693809724\n",
      "Coef/err : 2.4419447432394126\n",
      "=========================================================\n",
      "DRESSINGS/SALADS/PREP FOODS-DELI 8\n",
      "Coef : 0.033359352540150544\n",
      "Coef/err : 2.6534024155991034\n",
      "=========================================================\n",
      "DRESSINGS/SALADS/PREP FOODS-DELI 9\n",
      "Coef : 0.03300250079108635\n",
      "Coef/err : 2.619685845027355\n",
      "=========================================================\n",
      "DRESSINGS/SALADS/PREP FOODS-DELI 10\n",
      "Coef : 0.035474511270329634\n",
      "Coef/err : 2.810248919701369\n",
      "=========================================================\n",
      "DRESSINGS/SALADS/PREP FOODS-DELI 11\n",
      "Coef : 0.033453790248780146\n",
      "Coef/err : 2.664751479022158\n",
      "=========================================================\n",
      "DRESSINGS/SALADS/PREP FOODS-DELI 13\n",
      "Coef : 0.031142970983375484\n",
      "Coef/err : 2.4218477381195034\n",
      "=========================================================\n",
      "DRESSINGS/SALADS/PREP FOODS-DELI 15\n",
      "Coef : 0.0362391506900277\n",
      "Coef/err : 2.864289195734082\n",
      "=========================================================\n",
      "DRESSINGS/SALADS/PREP FOODS-DELI 16\n",
      "Coef : 0.03394880465279604\n",
      "Coef/err : 2.7133253378293998\n",
      "=========================================================\n",
      "DRESSINGS/SALADS/PREP FOODS-DELI 17\n",
      "Coef : 0.03423466221666227\n",
      "Coef/err : 2.7205192863458025\n",
      "=========================================================\n",
      "DRESSINGS/SALADS/PREP FOODS-DELI 18\n",
      "Coef : 0.03458963536972348\n",
      "Coef/err : 2.7652442672943884\n",
      "=========================================================\n",
      "DRESSINGS/SALADS/PREP FOODS-DELI 19\n",
      "Coef : 0.030171188536164184\n",
      "Coef/err : 2.385527517352367\n",
      "=========================================================\n",
      "DRESSINGS/SALADS/PREP FOODS-DELI 20\n",
      "Coef : 0.03495061474923422\n",
      "Coef/err : 2.7964610422856\n",
      "=========================================================\n",
      "DRESSINGS/SALADS/PREP FOODS-DELI 22\n",
      "Coef : 0.0342380983489452\n",
      "Coef/err : 2.721307074057802\n",
      "=========================================================\n",
      "DRESSINGS/SALADS/PREP FOODS-DELI 23\n",
      "Coef : 0.038486537890426664\n",
      "Coef/err : 3.030925912778669\n",
      "Size of the control group: 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\envs\\tr_econ\\lib\\site-packages\\statsmodels\\regression\\linear_model.py:1671: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  return np.dot(wresid, wresid) / self.df_resid\n",
      "C:\\anaconda\\envs\\tr_econ\\lib\\site-packages\\statsmodels\\regression\\linear_model.py:1671: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  return np.dot(wresid, wresid) / self.df_resid\n",
      "C:\\anaconda\\envs\\tr_econ\\lib\\site-packages\\statsmodels\\regression\\linear_model.py:1671: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  return np.dot(wresid, wresid) / self.df_resid\n",
      "C:\\anaconda\\envs\\tr_econ\\lib\\site-packages\\statsmodels\\regression\\linear_model.py:1671: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  return np.dot(wresid, wresid) / self.df_resid\n",
      "C:\\anaconda\\envs\\tr_econ\\lib\\site-packages\\statsmodels\\regression\\linear_model.py:1671: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  return np.dot(wresid, wresid) / self.df_resid\n",
      "C:\\anaconda\\envs\\tr_econ\\lib\\site-packages\\statsmodels\\regression\\linear_model.py:1671: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return np.dot(wresid, wresid) / self.df_resid\n",
      "C:\\anaconda\\envs\\tr_econ\\lib\\site-packages\\statsmodels\\regression\\linear_model.py:1671: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  return np.dot(wresid, wresid) / self.df_resid\n",
      "C:\\anaconda\\envs\\tr_econ\\lib\\site-packages\\statsmodels\\regression\\linear_model.py:1671: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  return np.dot(wresid, wresid) / self.df_resid\n",
      "C:\\anaconda\\envs\\tr_econ\\lib\\site-packages\\statsmodels\\base\\model.py:1512: RuntimeWarning: invalid value encountered in multiply\n",
      "  cov_p = self.normalized_cov_params * scale\n",
      "C:\\anaconda\\envs\\tr_econ\\lib\\site-packages\\statsmodels\\regression\\linear_model.py:1671: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return np.dot(wresid, wresid) / self.df_resid\n",
      "C:\\anaconda\\envs\\tr_econ\\lib\\site-packages\\statsmodels\\regression\\linear_model.py:1671: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  return np.dot(wresid, wresid) / self.df_resid\n",
      "C:\\anaconda\\envs\\tr_econ\\lib\\site-packages\\statsmodels\\regression\\linear_model.py:1671: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  return np.dot(wresid, wresid) / self.df_resid\n",
      "C:\\anaconda\\envs\\tr_econ\\lib\\site-packages\\statsmodels\\base\\model.py:1512: RuntimeWarning: invalid value encountered in multiply\n",
      "  cov_p = self.normalized_cov_params * scale\n",
      "C:\\anaconda\\envs\\tr_econ\\lib\\site-packages\\statsmodels\\regression\\linear_model.py:1671: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  return np.dot(wresid, wresid) / self.df_resid\n",
      "C:\\anaconda\\envs\\tr_econ\\lib\\site-packages\\statsmodels\\regression\\linear_model.py:1671: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  return np.dot(wresid, wresid) / self.df_resid\n",
      "C:\\anaconda\\envs\\tr_econ\\lib\\site-packages\\statsmodels\\base\\model.py:1512: RuntimeWarning: invalid value encountered in multiply\n",
      "  cov_p = self.normalized_cov_params * scale\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the control group: 334.\n",
      "=========================================================\n",
      "ICE CREAM, NOVELTIES 15\n",
      "Coef : -0.02278590174839501\n",
      "Coef/err : 2.4226508103497393\n",
      "Size of the control group: 205.\n",
      "=========================================================\n",
      "JAMS, JELLIES, SPREADS 1\n",
      "Coef : 0.04656582924028685\n",
      "Coef/err : 2.6661975578712624\n",
      "=========================================================\n",
      "JAMS, JELLIES, SPREADS 4\n",
      "Coef : 0.04877163024326747\n",
      "Coef/err : 2.548746183582311\n",
      "=========================================================\n",
      "JAMS, JELLIES, SPREADS 5\n",
      "Coef : 0.052336808750176544\n",
      "Coef/err : 2.988774350883568\n",
      "=========================================================\n",
      "JAMS, JELLIES, SPREADS 6\n",
      "Coef : 0.04098575309190888\n",
      "Coef/err : 2.33511963694018\n",
      "=========================================================\n",
      "JAMS, JELLIES, SPREADS 7\n",
      "Coef : 0.04879343761317134\n",
      "Coef/err : 2.854449022707696\n",
      "=========================================================\n",
      "JAMS, JELLIES, SPREADS 8\n",
      "Coef : 0.04852921182915981\n",
      "Coef/err : 2.823506245931434\n",
      "=========================================================\n",
      "JAMS, JELLIES, SPREADS 9\n",
      "Coef : 0.040350372368320286\n",
      "Coef/err : 2.278583594458927\n",
      "=========================================================\n",
      "JAMS, JELLIES, SPREADS 10\n",
      "Coef : 0.05451523926322688\n",
      "Coef/err : 3.0039777183876817\n",
      "=========================================================\n",
      "JAMS, JELLIES, SPREADS 11\n",
      "Coef : 0.05067424057519254\n",
      "Coef/err : 2.8460748627952053\n",
      "=========================================================\n",
      "JAMS, JELLIES, SPREADS 13\n",
      "Coef : 0.0531957733717896\n",
      "Coef/err : 3.0493408125083525\n",
      "=========================================================\n",
      "JAMS, JELLIES, SPREADS 15\n",
      "Coef : 0.04514035033929087\n",
      "Coef/err : 2.593888793102275\n",
      "=========================================================\n",
      "JAMS, JELLIES, SPREADS 16\n",
      "Coef : 0.05358123745194865\n",
      "Coef/err : 3.0182475544172775\n",
      "=========================================================\n",
      "JAMS, JELLIES, SPREADS 17\n",
      "Coef : 0.04868219580439592\n",
      "Coef/err : 2.6656398410200923\n",
      "=========================================================\n",
      "JAMS, JELLIES, SPREADS 18\n",
      "Coef : 0.05135109597618914\n",
      "Coef/err : 2.921714557648323\n",
      "=========================================================\n",
      "JAMS, JELLIES, SPREADS 19\n",
      "Coef : 0.04657785782857804\n",
      "Coef/err : 2.594832709951585\n",
      "=========================================================\n",
      "JAMS, JELLIES, SPREADS 20\n",
      "Coef : 0.046358216965187715\n",
      "Coef/err : 2.682752172844892\n",
      "=========================================================\n",
      "JAMS, JELLIES, SPREADS 22\n",
      "Coef : 0.04622705104218161\n",
      "Coef/err : 2.5191260396115585\n",
      "=========================================================\n",
      "JAMS, JELLIES, SPREADS 23\n",
      "Coef : 0.04701306367471503\n",
      "Coef/err : 2.68043302036431\n",
      "Size of the control group: 0.\n",
      "Size of the control group: 338.\n",
      "Size of the control group: 269.\n",
      "Size of the control group: 0.\n",
      "Size of the control group: 372.\n",
      "Size of the control group: 357.\n",
      "=========================================================\n",
      "VEGETABLES-FROZEN 13\n",
      "Coef : -0.023782187570335678\n",
      "Coef/err : 2.1336489409878143\n",
      "=========================================================\n",
      "VEGETABLES-FROZEN 15\n",
      "Coef : -0.024549998587561905\n",
      "Coef/err : 2.1510102569501637\n",
      "=========================================================\n",
      "VEGETABLES-FROZEN 16\n",
      "Coef : -0.023600399039077624\n",
      "Coef/err : 2.05589257094704\n",
      "Size of the control group: 177.\n"
     ]
    }
   ],
   "source": [
    "# We choose to focus on milk prices\n",
    "for category in categories:\n",
    "    #product_group = nielsen[nielsen.product_group_descr == category]\n",
    "    #product_group = pool\n",
    "    product_group = nielsen[nielsen.product_group_descr == category]\n",
    "\n",
    "    # The control group is composed by all states where nothing (no entry nor exit) happened.\n",
    "    control = product_group[~np.isin(product_group.guessed_store_county_fips, movements)].copy()\n",
    "    #print(f\"Size of the control group: {len(control.guessed_store_county_fips.unique())}.\")\n",
    "\n",
    "    # We keep in the control group the only counties where we have data for the entire time period (24 months)\n",
    "    nb_months = control.groupby('guessed_store_county_fips').count()\n",
    "    control = control[np.isin(control.guessed_store_county_fips, nb_months[nb_months.is_walmart==24].index)]\n",
    "    #print(f\"Size of the control group: {len(control.guessed_store_county_fips.unique())}.\")\n",
    "\n",
    "    nb_obs = control.groupby('guessed_store_county_fips').min()\n",
    "    control = control[np.isin(control.guessed_store_county_fips, nb_obs[nb_obs.nb_of_obs > 3].index)]\n",
    "    print(f\"Size of the control group: {len(control.guessed_store_county_fips.unique())}.\")\n",
    "    \n",
    "    control['time_effects'] = list(zip(control.purchase_year, control.purchase_month))\n",
    "\n",
    "\n",
    "    # The treatment group is composed by the states where one entry took place in 2016 and where this entry is the only movement\n",
    "    count = movements.groupby('County_fips').count()\n",
    "    count = count[count.State == 1] # No more than one movement in the treatement group\n",
    "    treatment_movements = movements[(np.isin(movements.County_fips, count.index))]\n",
    "    treatment_movements = treatment_movements[(treatment_movements.Opening_date>='2015-01-31' ) & (treatment_movements.Opening_date<='2017-01-31') & ((treatment_movements.Closing_date>'2017-01-31') | (treatment_movements.Closing_date.apply(str) == 'NaT'))]\n",
    "\n",
    "    treatment = product_group[np.isin(product_group.guessed_store_county_fips, treatment_movements.County_fips )].copy()\n",
    "    treatment = treatment.merge(treatment_movements, left_on='guessed_store_county_fips', right_on='County_fips')\n",
    "    #print(f\"Size of the treatment group: {len(treatment.guessed_store_county_fips.unique())}.\")\n",
    "\n",
    "\n",
    "    # We create our dummies for the regression\n",
    "    control['treat'] = False\n",
    "\n",
    "    treatment['treat'] = True\n",
    "    #treatment['interaction'] = ((treatment.Opening_date.dt.year == treatment.purchase_year) & (treatment.Opening_date.dt.month > treatment.purchase_month)) | (treatment.purchase_year > treatment.Opening_date.dt.year)\n",
    "    treatment['opening_0'] = treatment.Opening_date.dt.month  + 12 * (treatment.Opening_date.dt.year - 2015)\n",
    "    treatment['time_effects'] = list(zip(treatment.purchase_year, treatment.purchase_month))\n",
    "    \n",
    "    \n",
    "\n",
    "    for month in [1, 3, 4, 5, 6, 7, 8, 9 , 10, 11, 13, 15, 16, 17, 18, 19, 20, 22, 23]: #for month where at least one entry\n",
    "        # We keep only treated counties where the opening happened in month\n",
    "        treatment2 = treatment[treatment.opening_0 == month]\n",
    "\n",
    "        if (len(treatment2.guessed_store_county_fips.unique()) > 0) :\n",
    "\n",
    "            # Final dataset for the regression :\n",
    "            df = pd.concat((control, treatment2))[['upc_price', 'treat', 'purchase_month', 'purchase_year', 'time_effects', 'store_state']]\n",
    "            df = df[df.upc_price != 0]\n",
    "            df['purchase_0'] = df.purchase_month + 12 * (df.purchase_year - 2015)\n",
    "            df['post']=(df['purchase_0']>=month)\n",
    "            reg0 = smf.ols(formula='np.log(upc_price) ~ treat*post + C(time_effects) + C(store_state)', data=df)\n",
    "            results0 = reg0.fit()\n",
    "            if abs(results0.params[3] / results0.bse[3]) > 2. :\n",
    "            #if True:\n",
    "                print(\"=========================================================\")\n",
    "                print(category, month)\n",
    "                #print(results0.params)\n",
    "                print(f\"Coef : {np.exp(results0.params[3])-1}\")\n",
    "                print(f\"Coef/err : {abs(results0.params[3] / results0.bse[3])}\")\n",
    "                #print(f\"CI_up : {np.exp(results0.conf_int(alpha=0.05)[0][3])-1}\")\n",
    "                #print(f\"CI_down : {np.exp(results0.conf_int(alpha=0.05)[1][3])-1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREMIER MODELE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Prices_{i, t} = \\alpha + \\beta treat_i + \\gamma treat_i* post_t$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the control group: 1802.\n",
      "Size of the treatment group: 65.\n",
      "=========================================================\n",
      "_\n",
      "Coef : 0.012243573513392336\n",
      "Coef/err : 3.053990985459992\n",
      "CI_up : 0.0043688770208243355\n",
      "CI_down : 0.020180011111512908\n"
     ]
    }
   ],
   "source": [
    "# We choose to focus on milk prices\n",
    "for category in categories:\n",
    "    #product_group = nielsen[nielsen.product_group_descr == category]\n",
    "    #product_group = pool\n",
    "    product_group = nielsen\n",
    "\n",
    "    # The control group is composed by all states where nothing (no entry nor exit) happened.\n",
    "    control = product_group[~np.isin(product_group.guessed_store_county_fips, movements)].copy()\n",
    "    print(f\"Size of the control group: {len(control.guessed_store_county_fips.unique())}.\")\n",
    "\n",
    "\n",
    "    # The treatment group is composed by the states where one entry took place in 2016 and where this entry is the only movement\n",
    "    count = movements.groupby('County_fips').count()\n",
    "    count = count[count.State == 1] # No more than one movement in the treatement group\n",
    "    treatment_movements = movements[(np.isin(movements.County_fips, count.index))]\n",
    "    treatment_movements = treatment_movements[(treatment_movements.Opening_date>='2015-01-31' ) & (treatment_movements.Opening_date<='2017-01-31') & ((treatment_movements.Closing_date>'2017-01-31') | (treatment_movements.Closing_date.apply(str) == 'NaT'))]\n",
    "\n",
    "    treatment = product_group[np.isin(product_group.guessed_store_county_fips, treatment_movements.County_fips )].copy()\n",
    "    treatment = treatment.merge(treatment_movements, left_on='guessed_store_county_fips', right_on='County_fips')\n",
    "    print(f\"Size of the treatment group: {len(treatment.guessed_store_county_fips.unique())}.\")\n",
    "\n",
    "\n",
    "    # We create our dummies for the regression\n",
    "    control['treat'] = False\n",
    "    control['interaction'] = False\n",
    "\n",
    "    treatment['treat'] = True\n",
    "    #treatment['interaction'] = ((treatment.Opening_date.dt.year == treatment.purchase_year) & (treatment.Opening_date.dt.month > treatment.purchase_month)) | (treatment.purchase_year > treatment.Opening_date.dt.year)\n",
    "    treatment['purchase_0'] = treatment.purchase_month + 12 * (treatment.purchase_year - 2015)\n",
    "    treatment['opening_0'] = treatment.Opening_date.dt.month  + 12 * (treatment.Opening_date.dt.year - 2015)\n",
    "    treatment['interaction'] = treatment.purchase_0 >= treatment.opening_0 + post\n",
    "\n",
    "\n",
    "    # Final dataset for the regression :\n",
    "\n",
    "    df = pd.concat((control, treatment))[['upc_price', 'treat', 'interaction']]\n",
    "    df = df[df.upc_price != 0]\n",
    "    reg0 = smf.ols(formula='np.log(upc_price) ~ treat + interaction', data=df)\n",
    "    results0 = reg0.fit()\n",
    "    #if abs(results0.params[2] / results0.bse[2]) > 2. :\n",
    "    if True:\n",
    "        print(\"=========================================================\")\n",
    "        print(category)\n",
    "        print(f\"Coef : {np.exp(results0.params[2])-1}\")\n",
    "        print(f\"Coef/err : {abs(results0.params[2] / results0.bse[2])}\")\n",
    "        print(f\"CI_up : {np.exp(results0.conf_int(alpha=0.05)[0][2])-1}\")\n",
    "        print(f\"CI_down : {np.exp(results0.conf_int(alpha=0.05)[1][2])-1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEUXIEME MODELE - monthly time fixed effects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the control group: 1802.\n",
      "Size of the treatment group: 65.\n",
      "=========================================================\n",
      "_\n",
      "Coef : 0.02223552240504789\n",
      "Coef/err : 1.4577683184459918\n",
      "CI_up : -0.0075483686655423154\n",
      "CI_down : 0.052913240579445686\n"
     ]
    }
   ],
   "source": [
    "# We choose to focus on milk prices\n",
    "for category in categories:\n",
    "    #product_group = nielsen[nielsen.product_group_descr == category]\n",
    "    product_group = pool\n",
    "\n",
    "    # The control group is composed by all states where nothing (no entry nor exit) happened.\n",
    "    control = product_group[~np.isin(product_group.guessed_store_county_fips, movements)].copy()\n",
    "    print(f\"Size of the control group: {len(control.guessed_store_county_fips.unique())}.\")\n",
    "\n",
    "\n",
    "    # The treatment group is composed by the states where one entry took place in 2016 and where this entry is the only movement\n",
    "    count = movements.groupby('County_fips').count()\n",
    "    count = count[count.State == 1] # No more than one movement in the treatement group\n",
    "    treatment_movements = movements[(np.isin(movements.County_fips, count.index))]\n",
    "    treatment_movements = treatment_movements[(treatment_movements.Opening_date>='2015-01-31' ) & (treatment_movements.Opening_date<='2017-01-31') & ((treatment_movements.Closing_date>'2017-01-31') | (treatment_movements.Closing_date.apply(str) == 'NaT'))]\n",
    "\n",
    "    treatment = product_group[np.isin(product_group.guessed_store_county_fips, treatment_movements.County_fips )].copy()\n",
    "    treatment = treatment.merge(treatment_movements, left_on='guessed_store_county_fips', right_on='County_fips')\n",
    "    print(f\"Size of the treatment group: {len(treatment.guessed_store_county_fips.unique())}.\")\n",
    "\n",
    "\n",
    "    # We create our dummies for the regression\n",
    "    control['treat'] = False\n",
    "    control['interaction'] = False\n",
    "    control['time_effects'] = list(zip(control.purchase_year, control.purchase_month))\n",
    "\n",
    "    treatment['treat'] = True\n",
    "    #treatment['interaction'] = ((treatment.Opening_date.dt.year == treatment.purchase_year) & (treatment.Opening_date.dt.month > treatment.purchase_month)) | (treatment.Opening_date.dt.year > treatment.purchase_year)\n",
    "    treatment['purchase_0'] = treatment.purchase_month + 12 * (treatment.purchase_year - 2015)\n",
    "    treatment['opening_0'] = treatment.Opening_date.dt.month  + 12 * (treatment.Opening_date.dt.year - 2015)\n",
    "    treatment['interaction'] = treatment.purchase_0 >= treatment.opening_0 + post\n",
    "    treatment['time_effects'] = list(zip(treatment.purchase_year, treatment.purchase_month))\n",
    "\n",
    "\n",
    "    # Final dataset for the regression :\n",
    "\n",
    "    df = pd.concat((control, treatment))[['upc_price', 'treat', 'interaction', 'store_state', 'time_effects']]\n",
    "    df = df[df.upc_price != 0]\n",
    "\n",
    "\n",
    "    reg1 = smf.ols(formula='np.log(upc_price) ~ treat + interaction + C(time_effects)', data=df)\n",
    "    results1 = reg1.fit()\n",
    "    if True :\n",
    "    #if  abs(results1.params[2] / results1.bse[2])> 2.:\n",
    "        print(\"=========================================================\")\n",
    "        print(category)\n",
    "        print(f\"Coef : {np.exp(results1.params[2])-1}\")\n",
    "        print(f\"Coef/err : {abs(results1.params[2] / results1.bse[2])}\")\n",
    "        print(f\"CI_up : {np.exp(results1.conf_int(alpha=0.05)[0][2])-1}\")\n",
    "        print(f\"CI_down : {np.exp(results1.conf_int(alpha=0.05)[1][2])-1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TROISIEME MODELE - state effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================\n",
      "_\n",
      "Coef : 0.03341797594021911\n",
      "Coef/err : 2.3002084834970784\n",
      "CI_up : 0.004873365074778313\n",
      "CI_down : 0.06277343007982616\n"
     ]
    }
   ],
   "source": [
    "# We choose to focus on milk prices\n",
    "for category in categories:\n",
    "    #product_group = nielsen[nielsen.product_group_descr == category]\n",
    "    product_group = pool\n",
    "\n",
    "    # The control group is composed by all states where nothing (no entry nor exit) happened.\n",
    "    control = product_group[~np.isin(product_group.guessed_store_county_fips, movements)].copy()\n",
    "    #print(f\"Size of the control group: {len(control.guessed_store_county_fips.unique())}.\")\n",
    "\n",
    "\n",
    "    # The treatment group is composed by the states where one entry took place in 2016 and where this entry is the only movement\n",
    "    count = movements.groupby('County_fips').count()\n",
    "    count = count[count.State == 1] # No more than one movement in the treatement group\n",
    "    treatment_movements = movements[(np.isin(movements.County_fips, count.index))]\n",
    "    treatment_movements = treatment_movements[(treatment_movements.Opening_date>='2015-01-31' ) & (treatment_movements.Opening_date<='2017-01-31') & ((treatment_movements.Closing_date>'2017-01-31') | (treatment_movements.Closing_date.apply(str) == 'NaT'))]\n",
    "\n",
    "    treatment = product_group[np.isin(product_group.guessed_store_county_fips, treatment_movements.County_fips )].copy()\n",
    "    treatment = treatment.merge(treatment_movements, left_on='guessed_store_county_fips', right_on='County_fips')\n",
    "    #print(f\"Size of the treatment group: {len(treatment.guessed_store_county_fips.unique())}.\")\n",
    "\n",
    "\n",
    "    # We create our dummies for the regression\n",
    "    control['treat'] = False\n",
    "    control['interaction'] = False\n",
    "    control['time_effects'] = list(zip(control.purchase_year, control.purchase_month))\n",
    "\n",
    "    treatment['treat'] = True\n",
    "    treatment['purchase_0'] = treatment.purchase_month + 12 * (treatment.purchase_year - 2015)\n",
    "    treatment['opening_0'] = treatment.Opening_date.dt.month  + 12 * (treatment.Opening_date.dt.year - 2015)\n",
    "    treatment['interaction'] = treatment.purchase_0 >= treatment.opening_0 + post\n",
    "    treatment['time_effects'] = list(zip(treatment.purchase_year, treatment.purchase_month))\n",
    "\n",
    "\n",
    "    # Final dataset for the regression :\n",
    "\n",
    "    df = pd.concat((control, treatment))[['upc_price', 'treat', 'interaction', 'store_state', 'time_effects']]\n",
    "    df = df[df.upc_price != 0]\n",
    "\n",
    "\n",
    "    reg1 = smf.ols(formula='np.log(upc_price) ~ treat + interaction + C(time_effects) + C(store_state)', data=df)\n",
    "    results1 = reg1.fit()\n",
    "    print(\"=========================================================\")\n",
    "    print(category)\n",
    "    if True:\n",
    "    #if abs(results1.params[2] / results1.bse[2]) >= 2.:\n",
    "        print(f\"Coef : {np.exp(results1.params[2])-1}\")\n",
    "        print(f\"Coef/err : {abs(results1.params[2] / results1.bse[2])}\")\n",
    "        print(f\"CI_up : {np.exp(results1.conf_int(alpha=0.05)[0][2])-1}\")\n",
    "        print(f\"CI_down : {np.exp(results1.conf_int(alpha=0.05)[1][2])-1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QUATRIEME MODELE - state*time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLe Kernel sâest bloquÃ© lors de lâexÃ©cution du code dans la cellule active ou une cellule prÃ©cÃ©dente. Veuillez vÃ©rifier le code dans la ou les cellules pour identifier une cause possible de lâÃ©chec. Cliquez <a href='https://aka.ms/vscodeJupyterKernelCrash'>ici</a> pour plus dâinformations. Pour plus dâinformations, consultez Jupyter <a href='command:jupyter.viewOutput'>log</a>."
     ]
    }
   ],
   "source": [
    "# We choose to focus on milk prices\n",
    "for category in categories:\n",
    "    #product_group = nielsen[nielsen.product_group_descr == category]\n",
    "    #product_group = pool\n",
    "    product_group = nielsen\n",
    "\n",
    "    # The control group is composed by all states where nothing (no entry nor exit) happened.\n",
    "    control = product_group[~np.isin(product_group.guessed_store_county_fips, movements)].copy()\n",
    "    #print(f\"Size of the control group: {len(control.guessed_store_county_fips.unique())}.\")\n",
    "\n",
    "\n",
    "    # The treatment group is composed by the states where one entry took place in 2016 and where this entry is the only movement\n",
    "    count = movements.groupby('County_fips').count()\n",
    "    count = count[count.State == 1] # No more than one movement in the treatement group\n",
    "    treatment_movements = movements[(np.isin(movements.County_fips, count.index))]\n",
    "    treatment_movements = treatment_movements[(treatment_movements.Opening_date>='2015-01-31' ) & (treatment_movements.Opening_date<='2017-01-31') & ((treatment_movements.Closing_date>'2017-01-31') | (treatment_movements.Closing_date.apply(str) == 'NaT'))]\n",
    "\n",
    "    treatment = product_group[np.isin(product_group.guessed_store_county_fips, treatment_movements.County_fips )].copy()\n",
    "    treatment = treatment.merge(treatment_movements, left_on='guessed_store_county_fips', right_on='County_fips')\n",
    "    #print(f\"Size of the treatment group: {len(treatment.guessed_store_county_fips.unique())}.\")\n",
    "\n",
    "\n",
    "    # We create our dummies for the regression\n",
    "    control['treat'] = False\n",
    "    control['interaction'] = False\n",
    "    control['time_effects'] = list(zip(control.purchase_year, control.purchase_month))\n",
    "\n",
    "    treatment['treat'] = True\n",
    "    treatment['purchase_0'] = treatment.purchase_month + 12 * (treatment.purchase_year - 2015)\n",
    "    treatment['opening_0'] = treatment.Opening_date.dt.month  + 12 * (treatment.Opening_date.dt.year - 2015)\n",
    "    treatment['interaction'] = treatment.purchase_0 >= treatment.opening_0 + post\n",
    "    treatment['time_effects'] = list(zip(treatment.purchase_year, treatment.purchase_month))\n",
    "\n",
    "\n",
    "    # Final dataset for the regression :\n",
    "\n",
    "    df = pd.concat((control, treatment))[['upc_price', 'treat', 'interaction', 'store_state', 'time_effects']]\n",
    "    df = df[df.upc_price != 0]\n",
    "\n",
    "\n",
    "    reg1 = smf.ols(formula='np.log(upc_price) ~ treat + interaction + C(time_effects) * C(store_state)', data=df)\n",
    "    results1 = reg1.fit()\n",
    "    print(\"=========================================================\")\n",
    "    print(category)\n",
    "    if abs(results1.params[2] / results1.bse[2]) >= 2.:\n",
    "        print(f\"Coef : {np.exp(results1.params[2])-1}\")\n",
    "    print(f\"Coef/err : {abs(results1.params[2] / results1.bse[2])}\")\n",
    "    if abs(results1.params[2] / results1.bse[2]) >= 2.:\n",
    "        print(f\"CI_up : {np.exp(results1.conf_int(alpha=0.05)[0][2])-1}\")\n",
    "        print(f\"CI_down : {np.exp(results1.conf_int(alpha=0.05)[1][2])-1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the control group: 1751.\n",
      "Size of the control group: 701.\n",
      "Size of the control group: 460.\n",
      "Size of the treatment group: 65.\n",
      "=========================================================\n",
      "CANDY\n",
      "Coef : 0.058063436001930535\n",
      "Coef/err : 3.8703008634540486\n",
      "CI_up : 0.028246807350231995\n",
      "CI_down : 0.08874467355666482\n",
      "Size of the control group: 1753.\n",
      "Size of the control group: 974.\n",
      "Size of the control group: 660.\n",
      "Size of the treatment group: 65.\n",
      "=========================================================\n",
      "MILK\n",
      "Coef : -0.04109512362255707\n",
      "Coef/err : 5.162073822958096\n",
      "CI_up : -0.05625330864197686\n",
      "CI_down : -0.025693472241679283\n",
      "Size of the control group: 1610.\n",
      "Size of the control group: 450.\n",
      "Size of the control group: 246.\n",
      "Size of the treatment group: 65.\n",
      "=========================================================\n",
      "COFFEE\n",
      "Coef : 0.03699238687960982\n",
      "Coef/err : 2.9354635273784924\n",
      "CI_up : 0.012139500147311244\n",
      "CI_down : 0.0624555313667321\n",
      "Size of the control group: 1699.\n",
      "Size of the control group: 648.\n",
      "Size of the control group: 393.\n",
      "Size of the treatment group: 65.\n",
      "=========================================================\n",
      "PAPER PRODUCTS\n",
      "Coef : 0.10578211515236746\n",
      "Coef/err : 6.0283033645176305\n",
      "CI_up : 0.07021153549074\n",
      "CI_down : 0.14253495280272444\n",
      "Size of the control group: 1613.\n",
      "Size of the control group: 569.\n",
      "Size of the control group: 389.\n",
      "Size of the treatment group: 65.\n",
      "=========================================================\n",
      "YOGURT\n",
      "Coef : 0.06032904640608816\n",
      "Coef/err : 2.80193903855756\n",
      "CI_up : 0.01775344934561196\n",
      "CI_down : 0.10468570494684859\n"
     ]
    }
   ],
   "source": [
    "# We choose to focus on milk prices\n",
    "for category in ['CANDY', 'MILK', 'COFFEE', 'PAPER PRODUCTS', 'YOGURT']:\n",
    "    product_group = nielsen[nielsen.product_group_descr == category]\n",
    "    #product_group = pool\n",
    "    #product_group = nielsen\n",
    "\n",
    "    # The control group is composed by all states where nothing (no entry nor exit) happened.\n",
    "    control = product_group[~np.isin(product_group.guessed_store_county_fips, movements)].copy()\n",
    "    print(f\"Size of the control group: {len(control.guessed_store_county_fips.unique())}.\")\n",
    "\n",
    "    # We keep in the control group the only counties where we have data for the entire time period (24 months)\n",
    "    nb_months = control.groupby('guessed_store_county_fips').count()\n",
    "    control = control[np.isin(control.guessed_store_county_fips, nb_months[nb_months.is_walmart==24].index)]\n",
    "    print(f\"Size of the control group: {len(control.guessed_store_county_fips.unique())}.\")\n",
    "\n",
    "    nb_obs = control.groupby('guessed_store_county_fips').min()\n",
    "    control = control[np.isin(control.guessed_store_county_fips, nb_obs[nb_obs.nb_of_obs > 3].index)]\n",
    "    print(f\"Size of the control group: {len(control.guessed_store_county_fips.unique())}.\")\n",
    "\n",
    "    # The treatment group is composed by the states where one entry took place in 2016 and where this entry is the only movement\n",
    "    count = movements.groupby('County_fips').count()\n",
    "    count = count[count.State == 1] # No more than one movement in the treatement group\n",
    "    treatment_movements = movements[(np.isin(movements.County_fips, count.index))]\n",
    "    treatment_movements = treatment_movements[(treatment_movements.Opening_date>='2015-01-31' ) & (treatment_movements.Opening_date<='2017-01-31') & ((treatment_movements.Closing_date>'2017-01-31') | (treatment_movements.Closing_date.apply(str) == 'NaT'))]\n",
    "\n",
    "    treatment = product_group[np.isin(product_group.guessed_store_county_fips, treatment_movements.County_fips )].copy()\n",
    "    treatment = treatment.merge(treatment_movements, left_on='guessed_store_county_fips', right_on='County_fips')\n",
    "    print(f\"Size of the treatment group: {len(treatment.guessed_store_county_fips.unique())}.\")\n",
    "\n",
    "\n",
    "\n",
    "    # We create our dummies for the regression\n",
    "    control['treat'] = False\n",
    "    control['interaction'] = False\n",
    "    control['time_effects'] = list(zip(control.purchase_year, control.purchase_month))\n",
    "\n",
    "    treatment['treat'] = True\n",
    "    treatment['purchase_0'] = treatment.purchase_month + 12 * (treatment.purchase_year - 2015)\n",
    "    treatment['opening_0'] = treatment.Opening_date.dt.month  + 12 * (treatment.Opening_date.dt.year - 2015)\n",
    "    treatment['interaction'] = treatment.purchase_0 >= treatment.opening_0 + post\n",
    "    treatment['time_effects'] = list(zip(treatment.purchase_year, treatment.purchase_month))\n",
    "    treatment = treatment[abs(treatment.purchase_0-treatment.opening_0) >= 2]\n",
    "\n",
    "\n",
    "    # Final dataset for the regression :\n",
    "\n",
    "    df = pd.concat((control, treatment))[['upc_price', 'treat', 'interaction', 'store_state', 'time_effects']]\n",
    "    df = df[df.upc_price != 0]\n",
    "\n",
    "\n",
    "    reg1 = smf.ols(formula='np.log(upc_price) ~ treat + interaction + C(time_effects) * C(store_state)', data=df)\n",
    "    results1 = reg1.fit()\n",
    "    print(\"=========================================================\")\n",
    "    print(category)\n",
    "    if abs(results1.params[2] / results1.bse[2]) >= 2.:\n",
    "        print(f\"Coef : {np.exp(results1.params[2])-1}\")\n",
    "    print(f\"Coef/err : {abs(results1.params[2] / results1.bse[2])}\")\n",
    "    if abs(results1.params[2] / results1.bse[2]) >= 2.:\n",
    "        print(f\"CI_up : {np.exp(results1.conf_int(alpha=0.05)[0][2])-1}\")\n",
    "        print(f\"CI_down : {np.exp(results1.conf_int(alpha=0.05)[1][2])-1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "3d597f4c481aa0f25dceb95d2a0067e73c0966dcbd003d741d821a7208527ecf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
